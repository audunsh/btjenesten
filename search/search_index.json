{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"En virtuell h\u00e5ndsrekning fra B-tjenesten Studentene i B-tjenesten (beregningstjenesten) ved Kjemisk institutt, UiO bidrar til b\u00e5de godt milj\u00f8, veiledning og utvikling. Ved siden av v\u00e5r ukentlige veiledningstjeneste p\u00e5 Dalton (MU20) er vi involvert i programmering- og beregningsorientert innovasjon i undervisningen ved instituttet. Vi utvikler blant annet undervisningsmateriell, dokumentasjon, og kan bist\u00e5 med \u00e5 tilrettelegge eksisterende kode for undervisning. For \u00e5 im\u00f8tekomme en verden hvor nyvinninger br\u00e5tt kan skape nye behov og muligheter trenger vi dessuten ofte ny kode og tekniske l\u00f8sninger for undervisningen. V\u00e5re bidrag til dette finner du i v\u00e5r utstrakte virtuelle h\u00e5nd; Python-modulen btjenesten . Du kan installere oss med pip install btjenesten ...og importere oss med import btjenesten P\u00e5 disse sidene finner du dokumentasjon for v\u00e5r modul, samt litt informasjon om v\u00e5re aktiviteter. For mer informasjon, kom innom B-tjenesten p\u00e5 v\u00e5re veilednings\u00f8kter, torsdager 14.00-16.00 p\u00e5 Dalton (MU20) ved Kjemisk institutt, UiO. Du finner oss ogs\u00e5 p\u00e5 Facebook: B-tjenesten","title":"Hjem"},{"location":"#en-virtuell-handsrekning-fra-b-tjenesten","text":"Studentene i B-tjenesten (beregningstjenesten) ved Kjemisk institutt, UiO bidrar til b\u00e5de godt milj\u00f8, veiledning og utvikling. Ved siden av v\u00e5r ukentlige veiledningstjeneste p\u00e5 Dalton (MU20) er vi involvert i programmering- og beregningsorientert innovasjon i undervisningen ved instituttet. Vi utvikler blant annet undervisningsmateriell, dokumentasjon, og kan bist\u00e5 med \u00e5 tilrettelegge eksisterende kode for undervisning. For \u00e5 im\u00f8tekomme en verden hvor nyvinninger br\u00e5tt kan skape nye behov og muligheter trenger vi dessuten ofte ny kode og tekniske l\u00f8sninger for undervisningen. V\u00e5re bidrag til dette finner du i v\u00e5r utstrakte virtuelle h\u00e5nd; Python-modulen btjenesten . Du kan installere oss med pip install btjenesten ...og importere oss med import btjenesten P\u00e5 disse sidene finner du dokumentasjon for v\u00e5r modul, samt litt informasjon om v\u00e5re aktiviteter. For mer informasjon, kom innom B-tjenesten p\u00e5 v\u00e5re veilednings\u00f8kter, torsdager 14.00-16.00 p\u00e5 Dalton (MU20) ved Kjemisk institutt, UiO. Du finner oss ogs\u00e5 p\u00e5 Facebook: B-tjenesten","title":"En virtuell h\u00e5ndsrekning fra B-tjenesten"},{"location":"bakgrunn/","text":"","title":"Bakgrunn"},{"location":"bbtools-reference/","text":"Box-Behnken Tools (bbtools) tablewidget Tabular in/out for Box-Behnken widgets Author: Audun Skau Hansen, 2022 as_numpy_array ( self ) Returns the table (excluding headers) as a numpy array Source code in btjenesten/bbtools.py def as_numpy_array ( self ): \"\"\" Returns the table (excluding headers) as a numpy array \"\"\" ret = np . zeros ( self . tab [ 1 :, 1 :] . shape , dtype = float ) for i in range ( self . tab . shape [ 0 ] - 1 ): for j in range ( self . tab . shape [ 1 ] - 1 ): ret [ i , j ] = float ( self . tab [ i + 1 , j + 1 ] . value ) return ret set_from_array ( self , input_array ) Set the table (excluding headers) from an input array Source code in btjenesten/bbtools.py def set_from_array ( self , input_array ): \"\"\" Set the table (excluding headers) from an input array \"\"\" for i in range ( self . tab . shape [ 0 ] - 1 ): for j in range ( self . tab . shape [ 1 ] - 1 ): self . tab [ i + 1 , j + 1 ] . value = str ( input_array [ i , j ]) bbdesign ( n_center = 3 , randomize = True , sheet = None ) Returns a Box-Benhken experimental design for 3 variables Author: Audun Skau Hansen, Department of Chemistry, UiO Keyword arguments: Argument Description n_center number of samples in the center randomize whether or not to randomize the ordering (bool) sheet heet containing the min/max values of variables Source code in btjenesten/bbtools.py def bbdesign ( n_center = 3 , randomize = True , sheet = None ): \"\"\" Returns a Box-Benhken experimental design for 3 variables Author: Audun Skau Hansen, Department of Chemistry, UiO ## Keyword arguments: | Argument | Description | | ----------- | ----------- | | n_center | number of samples in the center | | randomize | whether or not to randomize the ordering (bool) | | sheet | heet containing the min/max values of variables | \"\"\" a = np . arange ( - 1 , 2 ) A = np . array ( np . meshgrid ( a , a , a )) . reshape ( 3 , - 1 ) . T A = np . concatenate ([ A [ np . sum ( A ** 2 , axis = 1 ) == 2 , :], np . zeros (( n_center , 3 ))]) ai = np . arange ( len ( A )) if randomize == True : # randomize run order np . random . shuffle ( ai ) if sheet is not None : # Transform coordinates tm = sheet . as_numpy_array () for i in range ( 3 ): A [:, i ] = interp1d ( np . linspace ( - 1 , 1 , 2 ), tm [ i ] )( A [:, i ]) #A = A.dot(tm) return A [ ai , :], ai bbsetup () Returns an interactive sheet (ipysheet) for setting up a Box-Benkhen design session Author : Audun Skau Hansen, Department of Chemistry, UiO Source code in btjenesten/bbtools.py def bbsetup (): \"\"\" Returns an interactive sheet (ipysheet) for setting up a Box-Benkhen design session **Author**: Audun Skau Hansen, Department of Chemistry, UiO \"\"\" global sheet arr = np . zeros (( 4 , 3 ), dtype = object ) arr [ 0 , 0 ] = \"\" arr [ 1 , 0 ] = \"Variable A\" arr [ 2 , 0 ] = \"Variable B\" arr [ 3 , 0 ] = \"Variable C\" arr [ 0 , 1 ] = \"Minimum\" arr [ 0 , 2 ] = \"Maximum\" arr [ 1 :, 1 ] = - 1 arr [ 1 :, 2 ] = 1 sheet = from_array ( arr ) sheet . column_headers = False sheet . row_headers = False return sheet bbsheet ( sheet ) Returns a Box-Behnken sheet for gathering experimental results Author: Audun Skau Hansen, Department of Chemistry, UiO Keyword arguments sheet = setup from bbsetup Source code in btjenesten/bbtools.py def bbsheet ( sheet ): \"\"\" Returns a Box-Behnken sheet for gathering experimental results Author: Audun Skau Hansen, Department of Chemistry, UiO ## Keyword arguments sheet = setup from bbsetup \"\"\" bd , ai = bbdesign ( sheet = sheet ) #global bb_sheet #sh = sheet.as_numpy_array() #arr = np.zeros(sh.shape + np.array([1,2]), dtype = object) #print(\"sh\") #print(sh) #print(bd) column_headers = [ \"Run\" , sheet . row_headers [ 0 ], sheet . row_headers [ 1 ], sheet . row_headers [ 2 ], \"Result\" ] row_headers = [ \"\" for i in range ( bd . shape [ 0 ])] bb_widget = tablewidget ( column_headers , row_headers ) arr = np . zeros ( ( len ( row_headers ), len ( column_headers )), dtype = float ) arr [:, 0 ] = np . arange ( len (( row_headers ))) + 1 arr [:, 0 ] = ai + 1 arr [:, 1 : 4 ] = bd bb_widget . set_from_array ( arr ) return bb_widget minitable ( titles , values , sheet ) Generate a mini-table for displaying inter-variable dependencies as indicated by the model Author: Audun Skau Hansen, Department of Chemistry, UiO Keyword arguments: Argument Description titles default variables values coefficients sheet the Box-Behnken sheet Source code in btjenesten/bbtools.py def minitable ( titles , values , sheet ): \"\"\" Generate a mini-table for displaying inter-variable dependencies as indicated by the model Author: Audun Skau Hansen, Department of Chemistry, UiO ## Keyword arguments: | Argument | Description | | ----------- | ----------- | | titles | default variables | | values | coefficients | | sheet | the Box-Behnken sheet | \"\"\" arr = np . zeros (( len ( titles ), 2 ), dtype = object ) arr [:, 0 ] = relabel_defaults ( titles , sheet ) arr [:, 1 ] = values return from_array ( arr ) relabel_defaults ( titles , new_names ) Rename default variable (\"x0\", \"x1\", \"x2\") to variable names from sheet[1:,0] Author : Audun Skau Hansen, Department of Chemistry, UiO Source code in btjenesten/bbtools.py def relabel_defaults ( titles , new_names ): \"\"\" Rename default variable (\"x0\", \"x1\", \"x2\") to variable names from sheet[1:,0] **Author**: Audun Skau Hansen, Department of Chemistry, UiO \"\"\" #new_names = to_array(sheet)[1:4,0] new_titles = [] for i in titles : new_titles . append ( i . replace ( \"x0\" , new_names [ 0 ]) . replace ( \"x1\" , new_names [ 1 ]) . replace ( \"x2\" , new_names [ 2 ]) ) return new_titles visualize_surfaces ( bbwidget , Nx = 30 ) Visualize response surfaces the regressor model Author : Audun Skau Hansen, Department of Chemistry, UiO (2022) Keyword arguments: Argument Description sheet Box-Benhken data sheet regressor sklearn LinearRegression instance Nx mesh resolution along each axis Source code in btjenesten/bbtools.py def visualize_surfaces ( bbwidget , Nx = 30 ): \"\"\" Visualize response surfaces the regressor model **Author**: Audun Skau Hansen, Department of Chemistry, UiO (2022) ## Keyword arguments: | Argument | Description | | ----------- | ----------- | | sheet | Box-Benhken data sheet | | regressor | sklearn LinearRegression instance | | Nx | mesh resolution along each axis | \"\"\" data = bbwidget . as_numpy_array () bounds = np . zeros (( 3 , 2 ), dtype = float ) bounds [:, 0 ] = np . min ( data [:, 1 : 4 ], axis = 0 ) bounds [:, 1 ] = np . max ( data [:, 1 : 4 ], axis = 0 ) #if regressor is None: # crop data from the sheet above X_train = data [:, 1 : 4 ] y_train = data [:, 4 ] #print(X_train, y_train) # perform a second order polynomial fit # (linear in the matrix elements) degree = 2 # second order poly = PolynomialFeatures ( degree ) # these are the matrix elements regressor = make_pipeline ( poly , LinearRegression ()) #set up the regressor regressor . fit ( X_train , y_train ) # fit the model # first, we extract all relevant information coefficients = regressor . steps [ 1 ][ 1 ] . coef_ names = poly . get_feature_names () predicted = regressor . predict ( X_train ) measured = y_train score = regressor . score ( X_train , y_train ) # we then compute and tabulate various statistics squared_error = ( predicted - measured ) ** 2 #print(predicted, measured) mean_squared_error = np . mean ( squared_error ) variance_error = np . var ( squared_error ) std_error = np . std ( squared_error ) # find max and min inside bounds using scipy.optimize from scipy.optimize import minimize mx = minimize ( lambda x : - 1 * regressor . predict ( np . array ([ x ])), X_train [ 0 ], bounds = bounds ) max_point = mx . x max_fun = - 1 * mx . fun mn = minimize ( lambda x : regressor . predict ( np . array ([ x ])), X_train [ 0 ], bounds = bounds ) min_point = mn . x min_fun = mn . fun #mse = mean_squared_error(predicted, measured) # mean squared error #mse = np.sum((predicted - measured)**2)/len(predicted) #alternative calculation print ( \"Mean squared error :\" , mean_squared_error ) print ( \"Variance of error :\" , variance_error ) print ( \"Standard dev. error:\" , std_error ) print ( \"Fitting score. :\" , score ) print ( \"Maximum coords :\" , max_point ) print ( \"Maximum value. :\" , max_fun [ 0 ]) print ( \"Minimum coords :\" , min_point ) print ( \"Minimum value. :\" , min_fun [ 0 ]) xa = np . linspace ( bounds [ 0 , 0 ], bounds [ 0 , 1 ], Nx ) xb = np . linspace ( bounds [ 1 , 0 ], bounds [ 1 , 1 ], Nx ) xc = np . linspace ( bounds [ 2 , 0 ], bounds [ 2 , 1 ], Nx ) va , vb , vc = bbwidget . column_headers [ 1 ], bbwidget . column_headers [ 2 ], bbwidget . column_headers [ 3 ] # displaying the fitting parameters fnames = relabel_defaults ( poly . get_feature_names (), [ va , vb , vc ]) ax , fig = plt . subplots ( figsize = ( 9 , 5 )) plt . plot ( regressor . steps [ 1 ][ 1 ] . coef_ , \"s\" ) #fig.set_xticklabels(poly.get_feature_names()) for i in range ( len ( regressor . steps [ 1 ][ 1 ] . coef_ )): plt . text ( i + .1 , regressor . steps [ 1 ][ 1 ] . coef_ [ i ], fnames [ i ], ha = \"left\" , va = \"center\" ) plt . axhline ( 0 ) plt . title ( \"Fitting parameters\" ) plt . show () # print a table of the fitting parameters print ( np . array ([ fnames , regressor . steps [ 1 ][ 1 ] . coef_ ]) . T ) print ( \"Intercept:\" , regressor . steps [ 1 ][ 1 ] . intercept_ ) \"\"\" plt.figure(figsize=(9.5,8)) plt.title(va + \" vs \" + vb) plt.contourf(xa,xb,yab) plt.xlabel(va) plt.ylabel(vb) plt.colorbar() plt.show() \"\"\" \"\"\" plt.figure(figsize=(9.5,8)) plt.title(va + \" vs \" + vc) plt.contourf(xa,xc,yac) plt.xlabel(va) plt.ylabel(vc) plt.colorbar() plt.show() \"\"\" \"\"\" plt.figure(figsize=(9.5,8)) plt.title(vb + \" vs \" + vc) plt.contourf(xb,xc,ybc) plt.xlabel(vb) plt.ylabel(vc) plt.colorbar() plt.show() \"\"\" #fig = plt.figure(figsize=(9,3)) fig , ( ax1 , ax2 , ax3 ) = plt . subplots ( 1 , 3 , figsize = ( 9 , 4 ), sharey = True ) #fig. #ax = fig.add_subplot(1, 3, 1) Xa = np . zeros (( Nx , 3 ), dtype = float ) Xa [:, 0 ] = xa Xa [:, 1 :] = min_point [ 1 :] ax1 . plot ( xa , regressor . predict ( Xa )) ax1 . set_xlabel ( va ) #ax = fig.add_subplot(1, 3, 2) Xb = np . zeros (( Nx , 3 ), dtype = float ) Xb [:, 1 ] = xb Xb [:, 0 ] = min_point [ 0 ] Xb [:, 2 ] = min_point [ 2 ] ax2 . plot ( xb , regressor . predict ( Xb )) ax2 . set_xlabel ( vb ) ax2 . set_title ( \"Fitted means\" ) #ax = fig.add_subplot(1, 3, 3) Xc = np . zeros (( Nx , 3 ), dtype = float ) Xc [:, 2 ] = xc Xc [:, 0 ] = min_point [ 0 ] Xc [:, 1 ] = min_point [ 1 ] ax3 . plot ( xc , regressor . predict ( Xc )) ax3 . set_xlabel ( vc ) #ax.show() #plt.show() xab3 = np . vstack (( np . array ( np . meshgrid ( xa , xb )) . reshape ( 2 , - 1 ), np . zeros ( Nx ** 2 ))) . T yab = regressor . predict ( xab3 ) . reshape (( Nx , Nx )) #fig, ax = plt.subplots() #subplot_kw={\"projection\": \"3d\"}) fig = plt . figure ( figsize = ( 9 , 3 )) ax = fig . add_subplot ( 1 , 3 , 1 , projection = '3d' ) X , Y = np . meshgrid ( xa , xb ) surf = ax . plot_surface ( X , Y , yab , linewidth = 0 , antialiased = False , cmap = cm . coolwarm ) #fig.colorbar(surf, shrink=0.3, aspect=5) ax . contour ( X , Y , yab , zdir = 'z' , offset = yab . min (), cmap = cm . coolwarm ) plt . xlabel ( va ) plt . ylabel ( vb ) #plt.show() xac3 = np . vstack (( np . array ( np . meshgrid ( xa , xc )) . reshape ( 2 , - 1 ), np . zeros ( Nx ** 2 ))) . T xac3 [:,[ 1 , 2 ]] = xac3 [:, [ 2 , 1 ]] yac = regressor . predict ( xac3 ) . reshape (( Nx , Nx )) ax = fig . add_subplot ( 1 , 3 , 2 , projection = '3d' ) X , Y = np . meshgrid ( xa , xc ) surf = ax . plot_surface ( X , Y , yac , linewidth = 0 , antialiased = False , cmap = cm . coolwarm ) #fig.colorbar(surf, shrink=0.3, aspect=5) ax . contour ( X , Y , yac , zdir = 'z' , offset = yac . min (), cmap = cm . coolwarm ) plt . xlabel ( va ) plt . ylabel ( vc ) #plt.show() xbc3 = np . vstack (( np . array ( np . meshgrid ( xb , xc )) . reshape ( 2 , - 1 ), np . zeros ( Nx ** 2 ))) . T xbc3 [:,[ 0 , 1 , 2 ]] = xac3 [:, [ 1 , 0 , 2 ]] ybc = regressor . predict ( xbc3 ) . reshape (( Nx , Nx )) ax = fig . add_subplot ( 1 , 3 , 3 , projection = '3d' ) X , Y = np . meshgrid ( xb , xc ) surf = ax . plot_surface ( X , Y , ybc , linewidth = 0 , antialiased = False , cmap = cm . coolwarm ) #fig.colorbar(surf, shrink=0.3, aspect=5) ax . contour ( X , Y , ybc , zdir = 'z' , offset = ybc . min (), cmap = cm . coolwarm ) plt . xlabel ( vb ) plt . ylabel ( vc ) plt . show ()","title":"bbtools"},{"location":"bbtools-reference/#box-behnken-tools-bbtools","text":"","title":"Box-Behnken Tools (bbtools)"},{"location":"bbtools-reference/#btjenesten.bbtools.tablewidget","text":"Tabular in/out for Box-Behnken widgets Author: Audun Skau Hansen, 2022","title":"tablewidget"},{"location":"bbtools-reference/#btjenesten.bbtools.tablewidget.as_numpy_array","text":"Returns the table (excluding headers) as a numpy array Source code in btjenesten/bbtools.py def as_numpy_array ( self ): \"\"\" Returns the table (excluding headers) as a numpy array \"\"\" ret = np . zeros ( self . tab [ 1 :, 1 :] . shape , dtype = float ) for i in range ( self . tab . shape [ 0 ] - 1 ): for j in range ( self . tab . shape [ 1 ] - 1 ): ret [ i , j ] = float ( self . tab [ i + 1 , j + 1 ] . value ) return ret","title":"as_numpy_array()"},{"location":"bbtools-reference/#btjenesten.bbtools.tablewidget.set_from_array","text":"Set the table (excluding headers) from an input array Source code in btjenesten/bbtools.py def set_from_array ( self , input_array ): \"\"\" Set the table (excluding headers) from an input array \"\"\" for i in range ( self . tab . shape [ 0 ] - 1 ): for j in range ( self . tab . shape [ 1 ] - 1 ): self . tab [ i + 1 , j + 1 ] . value = str ( input_array [ i , j ])","title":"set_from_array()"},{"location":"bbtools-reference/#btjenesten.bbtools.bbdesign","text":"Returns a Box-Benhken experimental design for 3 variables Author: Audun Skau Hansen, Department of Chemistry, UiO","title":"bbdesign()"},{"location":"bbtools-reference/#btjenesten.bbtools.bbdesign--keyword-arguments","text":"Argument Description n_center number of samples in the center randomize whether or not to randomize the ordering (bool) sheet heet containing the min/max values of variables Source code in btjenesten/bbtools.py def bbdesign ( n_center = 3 , randomize = True , sheet = None ): \"\"\" Returns a Box-Benhken experimental design for 3 variables Author: Audun Skau Hansen, Department of Chemistry, UiO ## Keyword arguments: | Argument | Description | | ----------- | ----------- | | n_center | number of samples in the center | | randomize | whether or not to randomize the ordering (bool) | | sheet | heet containing the min/max values of variables | \"\"\" a = np . arange ( - 1 , 2 ) A = np . array ( np . meshgrid ( a , a , a )) . reshape ( 3 , - 1 ) . T A = np . concatenate ([ A [ np . sum ( A ** 2 , axis = 1 ) == 2 , :], np . zeros (( n_center , 3 ))]) ai = np . arange ( len ( A )) if randomize == True : # randomize run order np . random . shuffle ( ai ) if sheet is not None : # Transform coordinates tm = sheet . as_numpy_array () for i in range ( 3 ): A [:, i ] = interp1d ( np . linspace ( - 1 , 1 , 2 ), tm [ i ] )( A [:, i ]) #A = A.dot(tm) return A [ ai , :], ai","title":"Keyword arguments:"},{"location":"bbtools-reference/#btjenesten.bbtools.bbsetup","text":"Returns an interactive sheet (ipysheet) for setting up a Box-Benkhen design session Author : Audun Skau Hansen, Department of Chemistry, UiO Source code in btjenesten/bbtools.py def bbsetup (): \"\"\" Returns an interactive sheet (ipysheet) for setting up a Box-Benkhen design session **Author**: Audun Skau Hansen, Department of Chemistry, UiO \"\"\" global sheet arr = np . zeros (( 4 , 3 ), dtype = object ) arr [ 0 , 0 ] = \"\" arr [ 1 , 0 ] = \"Variable A\" arr [ 2 , 0 ] = \"Variable B\" arr [ 3 , 0 ] = \"Variable C\" arr [ 0 , 1 ] = \"Minimum\" arr [ 0 , 2 ] = \"Maximum\" arr [ 1 :, 1 ] = - 1 arr [ 1 :, 2 ] = 1 sheet = from_array ( arr ) sheet . column_headers = False sheet . row_headers = False return sheet","title":"bbsetup()"},{"location":"bbtools-reference/#btjenesten.bbtools.bbsheet","text":"Returns a Box-Behnken sheet for gathering experimental results Author: Audun Skau Hansen, Department of Chemistry, UiO","title":"bbsheet()"},{"location":"bbtools-reference/#btjenesten.bbtools.bbsheet--keyword-arguments","text":"sheet = setup from bbsetup Source code in btjenesten/bbtools.py def bbsheet ( sheet ): \"\"\" Returns a Box-Behnken sheet for gathering experimental results Author: Audun Skau Hansen, Department of Chemistry, UiO ## Keyword arguments sheet = setup from bbsetup \"\"\" bd , ai = bbdesign ( sheet = sheet ) #global bb_sheet #sh = sheet.as_numpy_array() #arr = np.zeros(sh.shape + np.array([1,2]), dtype = object) #print(\"sh\") #print(sh) #print(bd) column_headers = [ \"Run\" , sheet . row_headers [ 0 ], sheet . row_headers [ 1 ], sheet . row_headers [ 2 ], \"Result\" ] row_headers = [ \"\" for i in range ( bd . shape [ 0 ])] bb_widget = tablewidget ( column_headers , row_headers ) arr = np . zeros ( ( len ( row_headers ), len ( column_headers )), dtype = float ) arr [:, 0 ] = np . arange ( len (( row_headers ))) + 1 arr [:, 0 ] = ai + 1 arr [:, 1 : 4 ] = bd bb_widget . set_from_array ( arr ) return bb_widget","title":"Keyword arguments"},{"location":"bbtools-reference/#btjenesten.bbtools.minitable","text":"Generate a mini-table for displaying inter-variable dependencies as indicated by the model Author: Audun Skau Hansen, Department of Chemistry, UiO","title":"minitable()"},{"location":"bbtools-reference/#btjenesten.bbtools.minitable--keyword-arguments","text":"Argument Description titles default variables values coefficients sheet the Box-Behnken sheet Source code in btjenesten/bbtools.py def minitable ( titles , values , sheet ): \"\"\" Generate a mini-table for displaying inter-variable dependencies as indicated by the model Author: Audun Skau Hansen, Department of Chemistry, UiO ## Keyword arguments: | Argument | Description | | ----------- | ----------- | | titles | default variables | | values | coefficients | | sheet | the Box-Behnken sheet | \"\"\" arr = np . zeros (( len ( titles ), 2 ), dtype = object ) arr [:, 0 ] = relabel_defaults ( titles , sheet ) arr [:, 1 ] = values return from_array ( arr )","title":"Keyword arguments:"},{"location":"bbtools-reference/#btjenesten.bbtools.relabel_defaults","text":"Rename default variable (\"x0\", \"x1\", \"x2\") to variable names from sheet[1:,0] Author : Audun Skau Hansen, Department of Chemistry, UiO Source code in btjenesten/bbtools.py def relabel_defaults ( titles , new_names ): \"\"\" Rename default variable (\"x0\", \"x1\", \"x2\") to variable names from sheet[1:,0] **Author**: Audun Skau Hansen, Department of Chemistry, UiO \"\"\" #new_names = to_array(sheet)[1:4,0] new_titles = [] for i in titles : new_titles . append ( i . replace ( \"x0\" , new_names [ 0 ]) . replace ( \"x1\" , new_names [ 1 ]) . replace ( \"x2\" , new_names [ 2 ]) ) return new_titles","title":"relabel_defaults()"},{"location":"bbtools-reference/#btjenesten.bbtools.visualize_surfaces","text":"Visualize response surfaces the regressor model Author : Audun Skau Hansen, Department of Chemistry, UiO (2022)","title":"visualize_surfaces()"},{"location":"bbtools-reference/#btjenesten.bbtools.visualize_surfaces--keyword-arguments","text":"Argument Description sheet Box-Benhken data sheet regressor sklearn LinearRegression instance Nx mesh resolution along each axis Source code in btjenesten/bbtools.py def visualize_surfaces ( bbwidget , Nx = 30 ): \"\"\" Visualize response surfaces the regressor model **Author**: Audun Skau Hansen, Department of Chemistry, UiO (2022) ## Keyword arguments: | Argument | Description | | ----------- | ----------- | | sheet | Box-Benhken data sheet | | regressor | sklearn LinearRegression instance | | Nx | mesh resolution along each axis | \"\"\" data = bbwidget . as_numpy_array () bounds = np . zeros (( 3 , 2 ), dtype = float ) bounds [:, 0 ] = np . min ( data [:, 1 : 4 ], axis = 0 ) bounds [:, 1 ] = np . max ( data [:, 1 : 4 ], axis = 0 ) #if regressor is None: # crop data from the sheet above X_train = data [:, 1 : 4 ] y_train = data [:, 4 ] #print(X_train, y_train) # perform a second order polynomial fit # (linear in the matrix elements) degree = 2 # second order poly = PolynomialFeatures ( degree ) # these are the matrix elements regressor = make_pipeline ( poly , LinearRegression ()) #set up the regressor regressor . fit ( X_train , y_train ) # fit the model # first, we extract all relevant information coefficients = regressor . steps [ 1 ][ 1 ] . coef_ names = poly . get_feature_names () predicted = regressor . predict ( X_train ) measured = y_train score = regressor . score ( X_train , y_train ) # we then compute and tabulate various statistics squared_error = ( predicted - measured ) ** 2 #print(predicted, measured) mean_squared_error = np . mean ( squared_error ) variance_error = np . var ( squared_error ) std_error = np . std ( squared_error ) # find max and min inside bounds using scipy.optimize from scipy.optimize import minimize mx = minimize ( lambda x : - 1 * regressor . predict ( np . array ([ x ])), X_train [ 0 ], bounds = bounds ) max_point = mx . x max_fun = - 1 * mx . fun mn = minimize ( lambda x : regressor . predict ( np . array ([ x ])), X_train [ 0 ], bounds = bounds ) min_point = mn . x min_fun = mn . fun #mse = mean_squared_error(predicted, measured) # mean squared error #mse = np.sum((predicted - measured)**2)/len(predicted) #alternative calculation print ( \"Mean squared error :\" , mean_squared_error ) print ( \"Variance of error :\" , variance_error ) print ( \"Standard dev. error:\" , std_error ) print ( \"Fitting score. :\" , score ) print ( \"Maximum coords :\" , max_point ) print ( \"Maximum value. :\" , max_fun [ 0 ]) print ( \"Minimum coords :\" , min_point ) print ( \"Minimum value. :\" , min_fun [ 0 ]) xa = np . linspace ( bounds [ 0 , 0 ], bounds [ 0 , 1 ], Nx ) xb = np . linspace ( bounds [ 1 , 0 ], bounds [ 1 , 1 ], Nx ) xc = np . linspace ( bounds [ 2 , 0 ], bounds [ 2 , 1 ], Nx ) va , vb , vc = bbwidget . column_headers [ 1 ], bbwidget . column_headers [ 2 ], bbwidget . column_headers [ 3 ] # displaying the fitting parameters fnames = relabel_defaults ( poly . get_feature_names (), [ va , vb , vc ]) ax , fig = plt . subplots ( figsize = ( 9 , 5 )) plt . plot ( regressor . steps [ 1 ][ 1 ] . coef_ , \"s\" ) #fig.set_xticklabels(poly.get_feature_names()) for i in range ( len ( regressor . steps [ 1 ][ 1 ] . coef_ )): plt . text ( i + .1 , regressor . steps [ 1 ][ 1 ] . coef_ [ i ], fnames [ i ], ha = \"left\" , va = \"center\" ) plt . axhline ( 0 ) plt . title ( \"Fitting parameters\" ) plt . show () # print a table of the fitting parameters print ( np . array ([ fnames , regressor . steps [ 1 ][ 1 ] . coef_ ]) . T ) print ( \"Intercept:\" , regressor . steps [ 1 ][ 1 ] . intercept_ ) \"\"\" plt.figure(figsize=(9.5,8)) plt.title(va + \" vs \" + vb) plt.contourf(xa,xb,yab) plt.xlabel(va) plt.ylabel(vb) plt.colorbar() plt.show() \"\"\" \"\"\" plt.figure(figsize=(9.5,8)) plt.title(va + \" vs \" + vc) plt.contourf(xa,xc,yac) plt.xlabel(va) plt.ylabel(vc) plt.colorbar() plt.show() \"\"\" \"\"\" plt.figure(figsize=(9.5,8)) plt.title(vb + \" vs \" + vc) plt.contourf(xb,xc,ybc) plt.xlabel(vb) plt.ylabel(vc) plt.colorbar() plt.show() \"\"\" #fig = plt.figure(figsize=(9,3)) fig , ( ax1 , ax2 , ax3 ) = plt . subplots ( 1 , 3 , figsize = ( 9 , 4 ), sharey = True ) #fig. #ax = fig.add_subplot(1, 3, 1) Xa = np . zeros (( Nx , 3 ), dtype = float ) Xa [:, 0 ] = xa Xa [:, 1 :] = min_point [ 1 :] ax1 . plot ( xa , regressor . predict ( Xa )) ax1 . set_xlabel ( va ) #ax = fig.add_subplot(1, 3, 2) Xb = np . zeros (( Nx , 3 ), dtype = float ) Xb [:, 1 ] = xb Xb [:, 0 ] = min_point [ 0 ] Xb [:, 2 ] = min_point [ 2 ] ax2 . plot ( xb , regressor . predict ( Xb )) ax2 . set_xlabel ( vb ) ax2 . set_title ( \"Fitted means\" ) #ax = fig.add_subplot(1, 3, 3) Xc = np . zeros (( Nx , 3 ), dtype = float ) Xc [:, 2 ] = xc Xc [:, 0 ] = min_point [ 0 ] Xc [:, 1 ] = min_point [ 1 ] ax3 . plot ( xc , regressor . predict ( Xc )) ax3 . set_xlabel ( vc ) #ax.show() #plt.show() xab3 = np . vstack (( np . array ( np . meshgrid ( xa , xb )) . reshape ( 2 , - 1 ), np . zeros ( Nx ** 2 ))) . T yab = regressor . predict ( xab3 ) . reshape (( Nx , Nx )) #fig, ax = plt.subplots() #subplot_kw={\"projection\": \"3d\"}) fig = plt . figure ( figsize = ( 9 , 3 )) ax = fig . add_subplot ( 1 , 3 , 1 , projection = '3d' ) X , Y = np . meshgrid ( xa , xb ) surf = ax . plot_surface ( X , Y , yab , linewidth = 0 , antialiased = False , cmap = cm . coolwarm ) #fig.colorbar(surf, shrink=0.3, aspect=5) ax . contour ( X , Y , yab , zdir = 'z' , offset = yab . min (), cmap = cm . coolwarm ) plt . xlabel ( va ) plt . ylabel ( vb ) #plt.show() xac3 = np . vstack (( np . array ( np . meshgrid ( xa , xc )) . reshape ( 2 , - 1 ), np . zeros ( Nx ** 2 ))) . T xac3 [:,[ 1 , 2 ]] = xac3 [:, [ 2 , 1 ]] yac = regressor . predict ( xac3 ) . reshape (( Nx , Nx )) ax = fig . add_subplot ( 1 , 3 , 2 , projection = '3d' ) X , Y = np . meshgrid ( xa , xc ) surf = ax . plot_surface ( X , Y , yac , linewidth = 0 , antialiased = False , cmap = cm . coolwarm ) #fig.colorbar(surf, shrink=0.3, aspect=5) ax . contour ( X , Y , yac , zdir = 'z' , offset = yac . min (), cmap = cm . coolwarm ) plt . xlabel ( va ) plt . ylabel ( vc ) #plt.show() xbc3 = np . vstack (( np . array ( np . meshgrid ( xb , xc )) . reshape ( 2 , - 1 ), np . zeros ( Nx ** 2 ))) . T xbc3 [:,[ 0 , 1 , 2 ]] = xac3 [:, [ 1 , 0 , 2 ]] ybc = regressor . predict ( xbc3 ) . reshape (( Nx , Nx )) ax = fig . add_subplot ( 1 , 3 , 3 , projection = '3d' ) X , Y = np . meshgrid ( xb , xc ) surf = ax . plot_surface ( X , Y , ybc , linewidth = 0 , antialiased = False , cmap = cm . coolwarm ) #fig.colorbar(surf, shrink=0.3, aspect=5) ax . contour ( X , Y , ybc , zdir = 'z' , offset = ybc . min (), cmap = cm . coolwarm ) plt . xlabel ( vb ) plt . ylabel ( vc ) plt . show ()","title":"Keyword arguments:"},{"location":"btjenesten-reference/","text":"B-tjenesten","title":"btjenesten"},{"location":"btjenesten-reference/#b-tjenesten","text":"","title":"B-tjenesten"},{"location":"gpr-reference/","text":"Gaussian Process Regression tools (gpr) This module was developed by Elias Dalan in spring 2022. Kernel Kernel class covariance_function: The function that will be used to calculate the covariance between our datasets K ( self , X1 , X2 ) Function that returns the covariance matrix given our datasets X1: Dataset 1 (Often the training set) X2: Dataset 2 (Often the target set) self.covariance_function(X1, X2) : covariance matrix given our datasets X1 and X2. Source code in btjenesten/gpr.py def K ( self , X1 , X2 ): \"\"\" Function that returns the covariance matrix given our datasets Parameters: ----------- X1: Dataset 1 (Often the training set) X2: Dataset 2 (Often the target set) Returns: ---------- self.covariance_function(X1, X2) : covariance matrix given our datasets X1 and X2. \"\"\" if np . isscalar ( X1 ): X1 = np . array ([ X1 ]) if np . isscalar ( X2 ): X2 = np . array ([ X2 ]) return self . covariance_function ( X1 , X2 ) Regressor Gaussian process regressor class kernel: Specifies the type of covarince function we want for our regressor. If none is provided the default is the radial basis function training_data_X: Training data inputs, also called features training_data_Y: Training data outputs, also called labels aquisition ( self , minimize_prediction = True , x0 = None , l = 1.2 , delta = 0.1 ) Returns the point at which our model function is predicted to have the highest value. minimize_prediction: If your task is to minimize some model function, this parameter is True. If your task is to maximize the model function this parameter is False. l: Exploration parameter. Scales how much the standard deviation should impact the function value. l = 1 means that the function maximized/minimized equals predicted value +/- the standard deviation. x0: Initial guess. If not specified it will use the point at which the training data is the largest/smallest. delta: Hyperparameter that tunes UCB around measured datapoints. p - The predicted point at which an evaluation would yeild the highest/lowest value Source code in btjenesten/gpr.py def aquisition ( self , minimize_prediction = True , x0 = None , l = 1.2 , delta = 0.1 ): \"\"\" Returns the point at which our model function is predicted to have the highest value. Parameters: ----------- minimize_prediction: If your task is to minimize some model function, this parameter is True. If your task is to maximize the model function this parameter is False. l: Exploration parameter. Scales how much the standard deviation should impact the function value. l = 1 means that the function maximized/minimized equals predicted value +/- the standard deviation. x0: Initial guess. If not specified it will use the point at which the training data is the largest/smallest. delta: Hyperparameter that tunes UCB around measured datapoints. Returns: -------- p - The predicted point at which an evaluation would yeild the highest/lowest value \"\"\" if minimize_prediction : #Minimization process if x0 == None : x0_index = np . where ( self . training_data_Y == np . min ( self . training_data_Y )) x0 = self . training_data_X [ x0_index ] x0 = x0 + np . random . rand ( len ( x0 )) * 0 objective_function = lambda x , predict = self . predict : predict ( x ) std_x = lambda x , predict = self . predict : np . sqrt ( np . abs ( np . diag ( predict ( x , return_covariance = True )[ 1 ]))) objective_noise = lambda x , std = std_x : ( 1 - std ( x )) ** 2 * delta + std ( x ) UCB = lambda x , exploit = objective_function , explore = objective_noise : exploit ( x ) + l * explore ( x ) minimization = minimize ( UCB , x0 ) p = minimization . x return p else : #Maximization process if x0 == None : x0_index = np . where ( self . training_data_Y == np . max ( self . training_data_Y )) x0 = self . training_data_X [ x0_index ] x0 = x0 + + np . random . rand ( len ( x0 )) * 0 objective_function = lambda x , predict = self . predict : predict ( x ) std_x = lambda x , predict = self . predict : np . sqrt ( np . abs ( np . diag ( predict ( x , return_covariance = True )[ 1 ]))) objective_noise = lambda x , std = std_x : ( 1 - std ( x )) ** 2 * delta + std ( x ) UCB = lambda x , exploit = objective_function , explore = objective_noise : - 1 * ( exploit ( x ) + l * explore ( x )) minimization = minimize ( UCB , x0 ) p = minimization . x return p predict ( self , input_data_X , training_data_X = None , training_data_Y = None , return_variance = False ) Predicts output values for some input data given a set of training data input_data_X: Input features that the gpr will evaluate. training_data_X: training data inputs. training_data_Y: training data outputs. return_variance: Returns variance for each prediction if this is true predicted_y: Predicted output data given cooresponding input_data_X and a set of training data inputs and outputs (training_data_X, training_data_Y) predicted_covariance_matrix: Predicted variance for each point of predicted output. Source code in btjenesten/gpr.py def predict ( self , input_data_X , training_data_X = None , training_data_Y = None , return_variance = False ): \"\"\" Predicts output values for some input data given a set of training data Parameters: ----------- input_data_X: Input features that the gpr will evaluate. training_data_X: training data inputs. training_data_Y: training data outputs. return_variance: Returns variance for each prediction if this is true Returns: ----------- predicted_y: Predicted output data given cooresponding input_data_X and a set of training data inputs and outputs (training_data_X, training_data_Y) predicted_covariance_matrix: Predicted variance for each point of predicted output. \"\"\" #msg = \"Input array likely contains single sample. Reshape your data using array.reshape(1, -1) if that is the case.\" #assert input_data_X.ndim != 1 and len(input_data_X) != , msg if training_data_X == None or training_data_Y == None : K_11 = self . kernel . K ( self . training_data_X , self . training_data_X ) K_12 = self . kernel . K ( self . training_data_X , input_data_X ) K_21 = K_12 . T K_22 = self . kernel . K ( input_data_X , input_data_X ) assert ( np . linalg . det ( K_11 ) != 0 ), \"Singular matrix. Training data might have duplicates.\" KT = np . linalg . solve ( K_11 , K_12 ) . T predicted_y = KT . dot ( self . training_data_Y ) else : K_11 = self . kernel . K ( training_data_X , training_data_X ) K_12 = self . kernel . K ( training_data_X , input_data_X ) K_21 = self . kernel . K ( input_data_X , training_data_X ) K_22 = self . kernel . K ( input_data_X , input_data_X ) assert ( np . linalg . det ( K_11 ) != 0 ), \"Singular matrix. Training data might have duplicates.\" KT = np . linalg . solve ( K_11 , K_12 ) . T predicted_y = KT . dot ( training_data_Y ) predicted_y = predicted_y . ravel () if return_variance : predicted_variance = np . diag ( K_22 - KT @ K_12 ) y_var_negative = predicted_variance < 0 if np . any ( y_var_negative ): predicted_variance . setflags ( write = \"True\" ) print ( \"Some variance values were negative. Setting them to zero.\" ) predicted_variance [ y_var_negative ] = 0 return predicted_y , predicted_variance else : return predicted_y score ( self , input_data_X , input_data_Y ) Returns the average and maximum error of our predict method. input_data_X: input data that the gpr will predict corresponding output data to. input_data_Y: Corresponding true ouput data for input_data_X. avg_error - the average error between the predicted values and the true values max_error - the maximum error between the predicted values and the true values Source code in btjenesten/gpr.py def score ( self , input_data_X , input_data_Y ): \"\"\" Returns the average and maximum error of our predict method. Parameters: ----------- input_data_X: input data that the gpr will predict corresponding output data to. input_data_Y: Corresponding true ouput data for input_data_X. Returns: -------- avg_error - the average error between the predicted values and the true values max_error - the maximum error between the predicted values and the true values \"\"\" predicted_y = self . predict ( input_data_X ) avg_error = np . mean ( np . abs ( predicted_y - input_data_Y )) max_error = np . max ( np . abs ( predicted_y - input_data_Y )) return avg_error update ( self , new_X , new_Y , tol = 1e-06 ) Updates the training data in accordance to some newly measured data. new_X: Set of new features that have been measured. new_Y: Corresponding set of labels to new_X. tol: Tolerance which the training data set can differ from new points. If this is too low you may encounter singular covariance matrices. Source code in btjenesten/gpr.py def update ( self , new_X , new_Y , tol = 1e-6 ): \"\"\" Updates the training data in accordance to some newly measured data. Parameters: ----------- new_X: Set of new features that have been measured. new_Y: Corresponding set of labels to new_X. tol: Tolerance which the training data set can differ from new points. If this is too low you may encounter singular covariance matrices. \"\"\" for measurement in new_X : for i in range ( len ( self . training_data_X )): if ( np . abs ( measurement - self . training_data_X [ i ]) < tol ) . all (): print ( f \"The model has most likely converged! { measurement } already exists in the training set.\" ) return old_X_shape = self . training_data_X . shape old_Y_shape = len ( self . training_data_Y ) new_X_shape = np . array ( self . training_data_X . shape ) new_Y_shape = np . array ( self . training_data_Y . shape ) new_X_shape [ 0 ] += new_X . shape [ 0 ] new_Y_shape += len ( new_Y ) new_training_data_X = np . zeros ( new_X_shape ) new_training_data_Y = np . zeros ( new_Y_shape ) new_training_data_X [: - new_X . shape [ 0 ]] = self . training_data_X new_training_data_X [ - new_X . shape [ 0 ]:] = new_X new_training_data_Y [: - len ( new_Y )] = self . training_data_Y new_training_data_Y [ - len ( new_Y ):] = new_Y indexes = np . argsort ( new_training_data_X ) self . training_data_X = new_training_data_X [ indexes ] self . training_data_Y = new_training_data_Y [ indexes ] Constant ( X1 , X2 , k = 0.5 ) Kernel that returns a constant covariance value between \\(x_i\\) and \\(x_j\\) Useful if all values depend on eachother equally. X1: Dataset 1 X2: Dataset 2 k: constant that determines the covariance. A matrix with the same shape as our input data. All matrix elements have the value k. Source code in btjenesten/kernels.py def Constant ( X1 , X2 , k = 0.5 ): \"\"\" Kernel that returns a constant covariance value between $x_i$ and $x_j$ Useful if all values depend on eachother equally. Parameters: ----------- X1: Dataset 1 X2: Dataset 2 k: constant that determines the covariance. Returns: A matrix with the same shape as our input data. All matrix elements have the value k. \"\"\" return np . ones ( X1 . shape ) * k Funny_trigonometric ( X1 , X2 , k = 1 ) Kernel that I made only for fun. May work for extravagant datasets X1: Dataset 1 X2: Dataset 2 k: constant that determines the frequency of the trigonometric functions. A covariance matrix that might be a bit crazy. Source code in btjenesten/kernels.py def Funny_trigonometric ( X1 , X2 , k = 1 ): \"\"\" Kernel that I made only for fun. May work for extravagant datasets Parameters: ----------- X1: Dataset 1 X2: Dataset 2 k: constant that determines the frequency of the trigonometric functions. Returns: A covariance matrix that might be a bit crazy. \"\"\" return np . sin ( - k * ( x [:, None ] - y [ None ,:]) ** 2 ) - np . cos ( - k * ( x [:, None ] - y [ None ,:]) ** 2 ) RBF ( X1 , X2 , l = 1 ) Radial basis function of the form: $$ e^{-l * d(X_i,X_j)} $$ X1: Dataset 1 X2: Dataset 2 l: Length scale parameter. Can be adjusted to adjust the covariance between \\(x_i\\) and \\(x_j\\) . Increasing l will decrease the covariance, and vice verca. A matrix with the same shape as our input data, where the elemets are: \\(e^{-l*d(x_i, x_j)}\\) where \\(d(x_i, x_j)\\) is the difference between element \\(x_i\\) in X1 and element \\(x_j\\) in X2. Source code in btjenesten/kernels.py def RBF ( X1 , X2 , l = 1 ): \"\"\" Radial basis function of the form: $$ e^{-l * d(X_i,X_j)} $$ Parameters: ----------- X1: Dataset 1 X2: Dataset 2 l: Length scale parameter. Can be adjusted to adjust the covariance between $x_i$ and $x_j$. Increasing l will decrease the covariance, and vice verca. Returns: ----------- A matrix with the same shape as our input data, where the elemets are: $e^{-l*d(x_i, x_j)}$ where $d(x_i, x_j)$ is the difference between element $x_i$ in X1 and element $x_j$ in X2. \"\"\" d = np . sum (( X1 . reshape ( X1 . shape [ 0 ], - 1 )[:, None ] - X2 . reshape ( X2 . shape [ 0 ], - 1 )[ None ,]) ** 2 , axis = 2 ) return np . exp ( - l * d )","title":"gpr"},{"location":"gpr-reference/#gaussian-process-regression-tools-gpr","text":"This module was developed by Elias Dalan in spring 2022.","title":"Gaussian Process Regression tools (gpr)"},{"location":"gpr-reference/#btjenesten.gpr.Kernel","text":"Kernel class covariance_function: The function that will be used to calculate the covariance between our datasets","title":"Kernel"},{"location":"gpr-reference/#btjenesten.gpr.Kernel.K","text":"Function that returns the covariance matrix given our datasets X1: Dataset 1 (Often the training set) X2: Dataset 2 (Often the target set) self.covariance_function(X1, X2) : covariance matrix given our datasets X1 and X2. Source code in btjenesten/gpr.py def K ( self , X1 , X2 ): \"\"\" Function that returns the covariance matrix given our datasets Parameters: ----------- X1: Dataset 1 (Often the training set) X2: Dataset 2 (Often the target set) Returns: ---------- self.covariance_function(X1, X2) : covariance matrix given our datasets X1 and X2. \"\"\" if np . isscalar ( X1 ): X1 = np . array ([ X1 ]) if np . isscalar ( X2 ): X2 = np . array ([ X2 ]) return self . covariance_function ( X1 , X2 )","title":"K()"},{"location":"gpr-reference/#btjenesten.gpr.Regressor","text":"Gaussian process regressor class kernel: Specifies the type of covarince function we want for our regressor. If none is provided the default is the radial basis function training_data_X: Training data inputs, also called features training_data_Y: Training data outputs, also called labels","title":"Regressor"},{"location":"gpr-reference/#btjenesten.gpr.Regressor.aquisition","text":"Returns the point at which our model function is predicted to have the highest value. minimize_prediction: If your task is to minimize some model function, this parameter is True. If your task is to maximize the model function this parameter is False. l: Exploration parameter. Scales how much the standard deviation should impact the function value. l = 1 means that the function maximized/minimized equals predicted value +/- the standard deviation. x0: Initial guess. If not specified it will use the point at which the training data is the largest/smallest. delta: Hyperparameter that tunes UCB around measured datapoints. p - The predicted point at which an evaluation would yeild the highest/lowest value Source code in btjenesten/gpr.py def aquisition ( self , minimize_prediction = True , x0 = None , l = 1.2 , delta = 0.1 ): \"\"\" Returns the point at which our model function is predicted to have the highest value. Parameters: ----------- minimize_prediction: If your task is to minimize some model function, this parameter is True. If your task is to maximize the model function this parameter is False. l: Exploration parameter. Scales how much the standard deviation should impact the function value. l = 1 means that the function maximized/minimized equals predicted value +/- the standard deviation. x0: Initial guess. If not specified it will use the point at which the training data is the largest/smallest. delta: Hyperparameter that tunes UCB around measured datapoints. Returns: -------- p - The predicted point at which an evaluation would yeild the highest/lowest value \"\"\" if minimize_prediction : #Minimization process if x0 == None : x0_index = np . where ( self . training_data_Y == np . min ( self . training_data_Y )) x0 = self . training_data_X [ x0_index ] x0 = x0 + np . random . rand ( len ( x0 )) * 0 objective_function = lambda x , predict = self . predict : predict ( x ) std_x = lambda x , predict = self . predict : np . sqrt ( np . abs ( np . diag ( predict ( x , return_covariance = True )[ 1 ]))) objective_noise = lambda x , std = std_x : ( 1 - std ( x )) ** 2 * delta + std ( x ) UCB = lambda x , exploit = objective_function , explore = objective_noise : exploit ( x ) + l * explore ( x ) minimization = minimize ( UCB , x0 ) p = minimization . x return p else : #Maximization process if x0 == None : x0_index = np . where ( self . training_data_Y == np . max ( self . training_data_Y )) x0 = self . training_data_X [ x0_index ] x0 = x0 + + np . random . rand ( len ( x0 )) * 0 objective_function = lambda x , predict = self . predict : predict ( x ) std_x = lambda x , predict = self . predict : np . sqrt ( np . abs ( np . diag ( predict ( x , return_covariance = True )[ 1 ]))) objective_noise = lambda x , std = std_x : ( 1 - std ( x )) ** 2 * delta + std ( x ) UCB = lambda x , exploit = objective_function , explore = objective_noise : - 1 * ( exploit ( x ) + l * explore ( x )) minimization = minimize ( UCB , x0 ) p = minimization . x return p","title":"aquisition()"},{"location":"gpr-reference/#btjenesten.gpr.Regressor.predict","text":"Predicts output values for some input data given a set of training data input_data_X: Input features that the gpr will evaluate. training_data_X: training data inputs. training_data_Y: training data outputs. return_variance: Returns variance for each prediction if this is true predicted_y: Predicted output data given cooresponding input_data_X and a set of training data inputs and outputs (training_data_X, training_data_Y) predicted_covariance_matrix: Predicted variance for each point of predicted output. Source code in btjenesten/gpr.py def predict ( self , input_data_X , training_data_X = None , training_data_Y = None , return_variance = False ): \"\"\" Predicts output values for some input data given a set of training data Parameters: ----------- input_data_X: Input features that the gpr will evaluate. training_data_X: training data inputs. training_data_Y: training data outputs. return_variance: Returns variance for each prediction if this is true Returns: ----------- predicted_y: Predicted output data given cooresponding input_data_X and a set of training data inputs and outputs (training_data_X, training_data_Y) predicted_covariance_matrix: Predicted variance for each point of predicted output. \"\"\" #msg = \"Input array likely contains single sample. Reshape your data using array.reshape(1, -1) if that is the case.\" #assert input_data_X.ndim != 1 and len(input_data_X) != , msg if training_data_X == None or training_data_Y == None : K_11 = self . kernel . K ( self . training_data_X , self . training_data_X ) K_12 = self . kernel . K ( self . training_data_X , input_data_X ) K_21 = K_12 . T K_22 = self . kernel . K ( input_data_X , input_data_X ) assert ( np . linalg . det ( K_11 ) != 0 ), \"Singular matrix. Training data might have duplicates.\" KT = np . linalg . solve ( K_11 , K_12 ) . T predicted_y = KT . dot ( self . training_data_Y ) else : K_11 = self . kernel . K ( training_data_X , training_data_X ) K_12 = self . kernel . K ( training_data_X , input_data_X ) K_21 = self . kernel . K ( input_data_X , training_data_X ) K_22 = self . kernel . K ( input_data_X , input_data_X ) assert ( np . linalg . det ( K_11 ) != 0 ), \"Singular matrix. Training data might have duplicates.\" KT = np . linalg . solve ( K_11 , K_12 ) . T predicted_y = KT . dot ( training_data_Y ) predicted_y = predicted_y . ravel () if return_variance : predicted_variance = np . diag ( K_22 - KT @ K_12 ) y_var_negative = predicted_variance < 0 if np . any ( y_var_negative ): predicted_variance . setflags ( write = \"True\" ) print ( \"Some variance values were negative. Setting them to zero.\" ) predicted_variance [ y_var_negative ] = 0 return predicted_y , predicted_variance else : return predicted_y","title":"predict()"},{"location":"gpr-reference/#btjenesten.gpr.Regressor.score","text":"Returns the average and maximum error of our predict method. input_data_X: input data that the gpr will predict corresponding output data to. input_data_Y: Corresponding true ouput data for input_data_X. avg_error - the average error between the predicted values and the true values max_error - the maximum error between the predicted values and the true values Source code in btjenesten/gpr.py def score ( self , input_data_X , input_data_Y ): \"\"\" Returns the average and maximum error of our predict method. Parameters: ----------- input_data_X: input data that the gpr will predict corresponding output data to. input_data_Y: Corresponding true ouput data for input_data_X. Returns: -------- avg_error - the average error between the predicted values and the true values max_error - the maximum error between the predicted values and the true values \"\"\" predicted_y = self . predict ( input_data_X ) avg_error = np . mean ( np . abs ( predicted_y - input_data_Y )) max_error = np . max ( np . abs ( predicted_y - input_data_Y )) return avg_error","title":"score()"},{"location":"gpr-reference/#btjenesten.gpr.Regressor.update","text":"Updates the training data in accordance to some newly measured data. new_X: Set of new features that have been measured. new_Y: Corresponding set of labels to new_X. tol: Tolerance which the training data set can differ from new points. If this is too low you may encounter singular covariance matrices. Source code in btjenesten/gpr.py def update ( self , new_X , new_Y , tol = 1e-6 ): \"\"\" Updates the training data in accordance to some newly measured data. Parameters: ----------- new_X: Set of new features that have been measured. new_Y: Corresponding set of labels to new_X. tol: Tolerance which the training data set can differ from new points. If this is too low you may encounter singular covariance matrices. \"\"\" for measurement in new_X : for i in range ( len ( self . training_data_X )): if ( np . abs ( measurement - self . training_data_X [ i ]) < tol ) . all (): print ( f \"The model has most likely converged! { measurement } already exists in the training set.\" ) return old_X_shape = self . training_data_X . shape old_Y_shape = len ( self . training_data_Y ) new_X_shape = np . array ( self . training_data_X . shape ) new_Y_shape = np . array ( self . training_data_Y . shape ) new_X_shape [ 0 ] += new_X . shape [ 0 ] new_Y_shape += len ( new_Y ) new_training_data_X = np . zeros ( new_X_shape ) new_training_data_Y = np . zeros ( new_Y_shape ) new_training_data_X [: - new_X . shape [ 0 ]] = self . training_data_X new_training_data_X [ - new_X . shape [ 0 ]:] = new_X new_training_data_Y [: - len ( new_Y )] = self . training_data_Y new_training_data_Y [ - len ( new_Y ):] = new_Y indexes = np . argsort ( new_training_data_X ) self . training_data_X = new_training_data_X [ indexes ] self . training_data_Y = new_training_data_Y [ indexes ]","title":"update()"},{"location":"gpr-reference/#btjenesten.kernels.Constant","text":"Kernel that returns a constant covariance value between \\(x_i\\) and \\(x_j\\) Useful if all values depend on eachother equally. X1: Dataset 1 X2: Dataset 2 k: constant that determines the covariance. A matrix with the same shape as our input data. All matrix elements have the value k. Source code in btjenesten/kernels.py def Constant ( X1 , X2 , k = 0.5 ): \"\"\" Kernel that returns a constant covariance value between $x_i$ and $x_j$ Useful if all values depend on eachother equally. Parameters: ----------- X1: Dataset 1 X2: Dataset 2 k: constant that determines the covariance. Returns: A matrix with the same shape as our input data. All matrix elements have the value k. \"\"\" return np . ones ( X1 . shape ) * k","title":"Constant()"},{"location":"gpr-reference/#btjenesten.kernels.Funny_trigonometric","text":"Kernel that I made only for fun. May work for extravagant datasets X1: Dataset 1 X2: Dataset 2 k: constant that determines the frequency of the trigonometric functions. A covariance matrix that might be a bit crazy. Source code in btjenesten/kernels.py def Funny_trigonometric ( X1 , X2 , k = 1 ): \"\"\" Kernel that I made only for fun. May work for extravagant datasets Parameters: ----------- X1: Dataset 1 X2: Dataset 2 k: constant that determines the frequency of the trigonometric functions. Returns: A covariance matrix that might be a bit crazy. \"\"\" return np . sin ( - k * ( x [:, None ] - y [ None ,:]) ** 2 ) - np . cos ( - k * ( x [:, None ] - y [ None ,:]) ** 2 )","title":"Funny_trigonometric()"},{"location":"gpr-reference/#btjenesten.kernels.RBF","text":"Radial basis function of the form: $$ e^{-l * d(X_i,X_j)} $$ X1: Dataset 1 X2: Dataset 2 l: Length scale parameter. Can be adjusted to adjust the covariance between \\(x_i\\) and \\(x_j\\) . Increasing l will decrease the covariance, and vice verca. A matrix with the same shape as our input data, where the elemets are: \\(e^{-l*d(x_i, x_j)}\\) where \\(d(x_i, x_j)\\) is the difference between element \\(x_i\\) in X1 and element \\(x_j\\) in X2. Source code in btjenesten/kernels.py def RBF ( X1 , X2 , l = 1 ): \"\"\" Radial basis function of the form: $$ e^{-l * d(X_i,X_j)} $$ Parameters: ----------- X1: Dataset 1 X2: Dataset 2 l: Length scale parameter. Can be adjusted to adjust the covariance between $x_i$ and $x_j$. Increasing l will decrease the covariance, and vice verca. Returns: ----------- A matrix with the same shape as our input data, where the elemets are: $e^{-l*d(x_i, x_j)}$ where $d(x_i, x_j)$ is the difference between element $x_i$ in X1 and element $x_j$ in X2. \"\"\" d = np . sum (( X1 . reshape ( X1 . shape [ 0 ], - 1 )[:, None ] - X2 . reshape ( X2 . shape [ 0 ], - 1 )[ None ,]) ** 2 , axis = 2 ) return np . exp ( - l * d )","title":"RBF()"},{"location":"veiledere/","text":"Veilederne i B-tjenesten f\u00e5r pedagogisk oppl\u00e6ring gjennom L\u00e6ringsassistentprogrammet ved Universitetet i Oslo, og f\u00e5r verdifull praktisk erfaring med utviklerverkt\u00f8y som git, markdown og Python. .card { box-shadow: 0 4px 8px 0 rgba(0, 0, 0, 0.2); max-width: 200px; margin: 5px 5px 5px 5px; padding: 25px; text-align: center; float: left; } .float-container { border: 3px solid #fff; padding: 20px; } .title { color: grey; font-size: 18px; } button { border: none; outline: 0; display: inline-block; padding: 8px; color: white; background-color: #000; text-align: center; cursor: pointer; width: 100%; font-size: 18px; } a { text-decoration: none; font-size: 22px; color: black; } button:hover, a:hover { opacity: 0.7; } Navn Kjemi (bachelor) Veileder B Kjemi (bachelor) Navn Kjemi (bachelor) Veileder B Kjemi (bachelor) Navn Kjemi (bachelor) Veileder B Kjemi (bachelor) Navn Kjemi (bachelor) Veileder B Kjemi (bachelor)","title":"V\u00e5re veiledere"}]}